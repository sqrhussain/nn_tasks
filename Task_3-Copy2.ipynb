{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load nn17_ex3_main.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Import Data\n",
    "\n",
    "# plt.close('all')  # if you like\n",
    "\n",
    "# load dataset\n",
    "\n",
    "with open('isolet_crop_train.pkl', 'rb') as f:\n",
    "    train_data = pkl.load(f)\n",
    "\n",
    "with open('isolet_crop_test.pkl', 'rb') as f:\n",
    "    test_data = pkl.load(f)\n",
    "\n",
    "X_train, y_train = train_data\n",
    "X_test, y_test = test_data\n",
    "\n",
    "\n",
    "# normalize the data and check the results\n",
    "\n",
    "# ...\n",
    "\n",
    "#print(X_train.mean(axis=0))\n",
    "#print(X_train.var(axis=0))\n",
    "\n",
    "# split the data sets, etc.\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 6238\n",
      "Number of testing examples = 1559\n",
      "....\n",
      "Number of Training features = 300\n",
      "Number of Testing features = 300\n",
      "....\n",
      "Number of Training Classes = 26\n",
      "Number of Testing Classes = 26\n"
     ]
    }
   ],
   "source": [
    "# Get training Data statistics\n",
    "\n",
    "# Number of Training examples\n",
    "n_train = len(X_train)\n",
    "# Number of Testing examples\n",
    "n_test = len(X_test)\n",
    "# Number of Training features\n",
    "n_TrainFeatures = len(X_train[0])\n",
    "# Number of Testing features (should be equal to that of training)\n",
    "n_TestFeatures = len (X_test[0])\n",
    "# Number of Training classes\n",
    "n_TrainClasses =  len(np.unique(y_train))\n",
    "# Number of Testing Classes (should be equal to that of training)\n",
    "n_TestClasses = len(np.unique(y_test))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"....\")\n",
    "print(\"Number of Training features =\", n_TrainFeatures)\n",
    "print(\"Number of Testing features =\", n_TestFeatures)\n",
    "print(\"....\")\n",
    "print(\"Number of Training Classes =\", n_TrainClasses)\n",
    "print(\"Number of Testing Classes =\", n_TestClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  a) Make yourself familiar with the dataset. Normalize each feature to zero mean and unitvariance."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#rfrom sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "#X_train, y_train = train_data\n",
    "#X_test, y_test = test_data\n",
    "\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.fit_transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = train_data\n",
    "#X_test, y_test = test_data\n",
    "\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "\n",
    "# Training data normalization\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "\n",
    "# Testing data normalization\n",
    "X_test = (X_test - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train,y_train)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# First split in 70% -- 30%\n",
    "X_train2, X_temp, y_train2, y_temp  = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
    "\n",
    "#Split the 30% into half to get 15% -- 15% (from X_train)for validation and early stop\n",
    "X_validation, X_stop, y_validation, y_stop = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "n_input = len(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 300)\n",
      "(6238,)\n",
      "(4366, 300)\n",
      "(4366,)\n",
      "(936, 300)\n",
      "(936,)\n",
      "(936, 300)\n",
      "(936,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train2.shape)\n",
    "print(y_train2.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(X_stop.shape)\n",
    "print(y_stop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-out-of-K coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 300))\n",
    "y = tf.placeholder(tf.int32,(None))\n",
    "one_hot_y = tf.one_hot(y, 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Classifier(x,n_TrainFeatures, n_TrainClasses):\n",
    "    \n",
    "    print(\"In Traffic Sign Classifier.......................................................................... \\n\")\n",
    "    \n",
    "    n_hidden_layer1 =20 \n",
    "    \n",
    "    n_input1 = n_TrainFeatures #300\n",
    "    n_classes = n_TrainClasses #26\n",
    "    # n_input1, n_hidden_layer1 n_classes\n",
    "    \n",
    "    # Layer 1: Hidden layer\n",
    "    layer1_W = tf.Variable(tf.truncated_normal([n_input1, n_hidden_layer1]),name='weights_1')\n",
    "    layer1_b = tf.Variable(tf.zeros(n_hidden_layer1),name='bias_1')\n",
    "    \n",
    "    layer1_out = tf.add(tf.matmul(x,layer1_W),layer1_b)\n",
    "    layer1_out = tf.sigmoid(layer1_out)\n",
    "    \n",
    "    # Layer 2: Output layer\n",
    "    layer2_W = tf.Variable(tf.truncated_normal([n_hidden_layer1, n_classes]),name='weights_2')\n",
    "    layer2_b = tf.Variable(tf.zeros(n_classes),name='bias_2')\n",
    "    \n",
    "    layer2_out = tf.add(tf.matmul(layer1_out, layer2_W), layer2_b)\n",
    "    #layer2_out = tf.sigmoid(layer2_out)\n",
    "    \n",
    "    logits = layer2_out\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indecies = np.unique(y_train)\n",
    "indecies = indecies -1\n",
    "depth = 27\n",
    "onehot = tf.one_hot(y_train,depth)\n",
    "sess = tf.Session()\n",
    "temp =sess.run(onehot)\n",
    "y_train = np.delete(temp,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Traffic Sign Classifier.......................................................................... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rate = 0.1 # learning rate\n",
    "\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 40\n",
    "\n",
    "logits = Train_Classifier(x,n_TrainFeatures, n_TrainClasses)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer()\n",
    "#tf.train.GradientDescentOptimizer(learning_rate=rate)\n",
    "\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "#tf.train.RMSPropOptimizer(rate)\n",
    "#tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a =[1,2,3,4]\n",
    "b = [7,2,3,4]\n",
    "\n",
    "correct_prediction = tf.equal(a, b)\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(accuracy_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(one_hot_y,1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver =tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "Best so far = 0.06303418913267107 on iteration 0\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.050\n",
      "Best so far = 0.11004273682577997 on iteration 1\n",
      "Best so far = 0.13782051468315798 on iteration 2\n",
      "Best so far = 0.17841880418296552 on iteration 3\n",
      "Best so far = 0.21474358999831045 on iteration 4\n",
      "Best so far = 0.2692307682118864 on iteration 5\n",
      "Best so far = 0.3076923047630196 on iteration 6\n",
      "Best so far = 0.3226495737958158 on iteration 7\n",
      "Best so far = 0.3504273518283143 on iteration 8\n",
      "Best so far = 0.3707264966180182 on iteration 9\n",
      "Best so far = 0.3867521363700557 on iteration 10\n",
      "Best so far = 0.4017094017094017 on iteration 11\n",
      "Best so far = 0.41666666641194594 on iteration 12\n",
      "Best so far = 0.4252136731759096 on iteration 14\n",
      "Best so far = 0.4358974364068773 on iteration 15\n",
      "Best so far = 0.44230768925104386 on iteration 16\n",
      "Best so far = 0.44978632223911774 on iteration 17\n",
      "Best so far = 0.47115384488024264 on iteration 18\n",
      "Best so far = 0.47863247914192003 on iteration 19\n",
      "Best so far = 0.48076922924090654 on iteration 20\n",
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.504\n",
      "Best so far = 0.4914529873774602 on iteration 21\n",
      "Best so far = 0.492521366247764 on iteration 22\n",
      "Best so far = 0.4967948677193405 on iteration 23\n",
      "Best so far = 0.5042735032546215 on iteration 24\n",
      "Best so far = 0.5096153848701053 on iteration 28\n",
      "Best so far = 0.5160256402614789 on iteration 29\n",
      "Best so far = 0.5267094022188431 on iteration 31\n",
      "Best so far = 0.5277777744664086 on iteration 35\n",
      "Best so far = 0.5320512761927059 on iteration 37\n",
      "Best so far = 0.5320512787399129 on iteration 38\n",
      "Best so far = 0.5384615376973764 on iteration 39\n",
      "Best so far = 0.5427350406972771 on iteration 40\n",
      "EPOCH 41 ...\n",
      "Validation Accuracy = 0.544\n",
      "Best so far = 0.5438034157467704 on iteration 41\n",
      "Best so far = 0.5491452971075335 on iteration 43\n",
      "Best so far = 0.5544871761758103 on iteration 44\n",
      "Best so far = 0.5619658114563706 on iteration 46\n",
      "Best so far = 0.5630341816661705 on iteration 49\n",
      "Best so far = 0.5662393116543436 on iteration 51\n",
      "Best so far = 0.5705128169467306 on iteration 52\n",
      "Best so far = 0.5747863222391177 on iteration 53\n",
      "Best so far = 0.576923077687239 on iteration 55\n",
      "Best so far = 0.5779914524820116 on iteration 56\n",
      "Best so far = 0.5790598300787119 on iteration 57\n",
      "Best so far = 0.5822649539535881 on iteration 58\n",
      "Best so far = 0.5897435933096796 on iteration 59\n",
      "Best so far = 0.5929487184581593 on iteration 60\n",
      "EPOCH 61 ...\n",
      "Validation Accuracy = 0.569\n",
      "Best so far = 0.5972222161089253 on iteration 68\n",
      "Best so far = 0.5982905949792291 on iteration 74\n",
      "EPOCH 81 ...\n",
      "Validation Accuracy = 0.578\n",
      "Best so far = 0.5982905982905983 on iteration 81\n",
      "Best so far = 0.5993589735948123 on iteration 83\n",
      "Best so far = 0.6004273460970985 on iteration 92\n",
      "Best so far = 0.6025641035829854 on iteration 100\n",
      "EPOCH 101 ...\n",
      "Validation Accuracy = 0.588\n",
      "EPOCH 121 ...\n",
      "Validation Accuracy = 0.592\n",
      "EPOCH 141 ...\n",
      "Validation Accuracy = 0.587\n",
      "Best so far = 0.6047008511347648 on iteration 142\n",
      "Best so far = 0.6057692287314651 on iteration 152\n",
      "Best so far = 0.6057692300050687 on iteration 155\n",
      "EPOCH 161 ...\n",
      "Validation Accuracy = 0.585\n",
      "Best so far = 0.6068376050545619 on iteration 162\n",
      "Best so far = 0.614316232693501 on iteration 164\n",
      "Best so far = 0.6175213631911155 on iteration 167\n",
      "EPOCH 181 ...\n",
      "Validation Accuracy = 0.580\n",
      "EPOCH 201 ...\n",
      "Validation Accuracy = 0.590\n",
      "EPOCH 221 ...\n",
      "Validation Accuracy = 0.594\n",
      "Best so far = 0.6175213698138539 on iteration 225\n",
      "Best so far = 0.6196581237336509 on iteration 226\n",
      "EPOCH 241 ...\n",
      "Validation Accuracy = 0.580\n",
      "EPOCH 261 ...\n",
      "Validation Accuracy = 0.584\n",
      "Best so far = 0.6207264924151266 on iteration 265\n",
      "EPOCH 281 ...\n",
      "Validation Accuracy = 0.591\n",
      "EPOCH 301 ...\n",
      "Validation Accuracy = 0.590\n",
      "EPOCH 321 ...\n",
      "Validation Accuracy = 0.591\n",
      "EPOCH 341 ...\n",
      "Validation Accuracy = 0.594\n",
      "Best so far = 0.6217948710307096 on iteration 343\n",
      "Best so far = 0.6250000012736036 on iteration 344\n",
      "Best so far = 0.6271367513725901 on iteration 350\n",
      "EPOCH 361 ...\n",
      "Validation Accuracy = 0.596\n",
      "Best so far = 0.6292735004526937 on iteration 362\n",
      "EPOCH 381 ...\n",
      "Validation Accuracy = 0.595\n",
      "EPOCH 401 ...\n",
      "Validation Accuracy = 0.597\n",
      "EPOCH 421 ...\n",
      "Validation Accuracy = 0.591\n",
      "EPOCH 441 ...\n",
      "Validation Accuracy = 0.587\n",
      "EPOCH 461 ...\n",
      "Validation Accuracy = 0.587\n",
      "EPOCH 481 ...\n",
      "Validation Accuracy = 0.591\n",
      "EPOCH 501 ...\n",
      "Validation Accuracy = 0.587\n",
      "EPOCH 521 ...\n",
      "Validation Accuracy = 0.588\n",
      "EPOCH 541 ...\n",
      "Validation Accuracy = 0.591\n",
      "EPOCH 561 ...\n",
      "Validation Accuracy = 0.590\n",
      "EPOCH 581 ...\n",
      "Validation Accuracy = 0.582\n",
      "EPOCH 601 ...\n",
      "Validation Accuracy = 0.585\n",
      "EPOCH 621 ...\n",
      "Validation Accuracy = 0.587\n",
      "EPOCH 641 ...\n",
      "Validation Accuracy = 0.588\n",
      "EPOCH 661 ...\n",
      "Validation Accuracy = 0.583\n",
      "EPOCH 681 ...\n",
      "Validation Accuracy = 0.581\n",
      "EPOCH 701 ...\n",
      "Validation Accuracy = 0.574\n",
      "EPOCH 721 ...\n",
      "Validation Accuracy = 0.567\n",
      "EPOCH 741 ...\n",
      "Validation Accuracy = 0.569\n",
      "EPOCH 761 ...\n",
      "Validation Accuracy = 0.571\n",
      "EPOCH 781 ...\n",
      "Validation Accuracy = 0.569\n",
      "EPOCH 801 ...\n",
      "Validation Accuracy = 0.565\n",
      "EPOCH 821 ...\n",
      "Validation Accuracy = 0.569\n",
      "EPOCH 841 ...\n",
      "Validation Accuracy = 0.575\n",
      "EPOCH 861 ...\n",
      "Validation Accuracy = 0.575\n",
      "EPOCH 881 ...\n",
      "Validation Accuracy = 0.575\n",
      "EPOCH 901 ...\n",
      "Validation Accuracy = 0.573\n",
      "EPOCH 921 ...\n",
      "Validation Accuracy = 0.572\n",
      "EPOCH 941 ...\n",
      "Validation Accuracy = 0.573\n",
      "EPOCH 961 ...\n",
      "Validation Accuracy = 0.578\n",
      "EPOCH 981 ...\n",
      "Validation Accuracy = 0.571\n",
      "EPOCH 1001 ...\n",
      "Validation Accuracy = 0.573\n",
      "EPOCH 1021 ...\n",
      "Validation Accuracy = 0.569\n"
     ]
    }
   ],
   "source": [
    "save_file = './modelTest1.ckpt'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train2)\n",
    "    \n",
    "    TempValidat_acc_list = []\n",
    "    TempTrain_acc_list = []\n",
    "    TempEarlyStopping_acc_list = []\n",
    "    \n",
    "    validat_acc_list = []\n",
    "    train_acc_list = []\n",
    "    earlyStoping_acc_list = []\n",
    "    \n",
    "    earlyStoppingMax = -1\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        \"\"\"\n",
    "        if len(validat_acc_list) > 1:\n",
    "            if validat_acc_list[-1] < validat_acc_list[-2] - 1e-3:\n",
    "                print(\"#############STOP################\")\n",
    "                break\n",
    "        \"\"\"\n",
    "        X_train21, y_train21 = shuffle(X_train2,y_train2)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train21[offset:end], y_train21[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        validation_accuracy = evaluate(X_validation, y_validation)\n",
    "        training_accuracy = evaluate(X_train2, y_train2)\n",
    "        earlyStoping_accuracy = evaluate(X_stop,y_stop)\n",
    "        \n",
    "        if earlyStoppingMax < earlyStoping_accuracy:\n",
    "            earlyStoppingMax = earlyStoping_accuracy\n",
    "            print('Best so far = {} on iteration {}'.format(earlyStoppingMax,i))\n",
    "            saver.save(sess,save_file)\n",
    "            #print(sess.run(tempw))\n",
    "        \n",
    "        TempValidat_acc_list.append(validation_accuracy)\n",
    "        TempTrain_acc_list.append(training_accuracy)\n",
    "        TempEarlyStopping_acc_list.append(earlyStoping_accuracy)\n",
    "        \n",
    "        if i%20 == 0:\n",
    "            validat_acc_list.append(np.mean(TempValidat_acc_list))\n",
    "            TempValidat_acc_list = []\n",
    "            \n",
    "            train_acc_list.append(np.mean(TempTrain_acc_list))\n",
    "            TempTrain_acc_list = []           \n",
    "            \n",
    "            earlyStoping_acc_list.append(np.mean(TempEarlyStopping_acc_list))\n",
    "            TempEarlyStopping_acc_list = []  \n",
    "        \n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #saver.save(sess, save_file)\n",
    "    #print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validat_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(validat_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(earlyStoping_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(earlyStoping_acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.reset_default_graph()\n",
    "n_hidden_layer1 =20 \n",
    "\n",
    "n_input1 = n_TrainFeatures #300\n",
    "n_classes = n_TrainClasses #26\n",
    "# n_input1, n_hidden_layer1 n_classes\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "layer1_W = tf.Variable(tf.truncated_normal([n_input1, n_hidden_layer1]),name='weights_1')\n",
    "layer1_b = tf.Variable(tf.zeros(n_hidden_layer1),name='bias_1')\n",
    "\n",
    "\n",
    "# Layer 2: Output layer\n",
    "layer2_W = tf.Variable(tf.truncated_normal([n_hidden_layer1, n_classes]),name='weights_2')\n",
    "layer2_b = tf.Variable(tf.zeros(n_classes),name='bias_2')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    saver.restore(sess, save_file)\n",
    "    #evaluate(X_stop,y_stop);\n",
    "    temp = (sess.run(layer1_W))\n",
    "print('Loaded Weights successfully.')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "fig,ax_list = plt.subplots(1,2)\n",
    "ax_list[0].plot(train_error_list, color='blue', label='training', lw=2)\n",
    "ax_list[0].plot(test_error_list, color='green', label='test', lw=2)\n",
    "ax_list[1].plot(1- np.array(train_acc_list), color='blue', label='training', lw=2)\n",
    "ax_list[1].plot(1 - np.array(test_acc_list), color='green', label='test', lw=2)\n",
    "\n",
    "ax_list[0].set_title('Cross-entropy')\n",
    "ax_list[0].set_xlabel('Training epoch')\n",
    "ax_list[0].set_ylabel('Cross-entropy')\n",
    "ax_list[1].set_title('Accuracy')\n",
    "ax_list[1].set_xlabel('Training epoch')\n",
    "ax_list[1].set_ylabel('Accuracy')\n",
    "ax_list[0].legend(loc='best')\n",
    "ax_list[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = './Test2.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights))\n",
    "print('Save Bias: {}'.format(bias))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights))\n",
    "print('Load Bias: {}'.format(bias))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = './Test.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
